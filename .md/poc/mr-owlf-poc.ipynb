{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho24SSY4EEZw",
        "colab_type": "text"
      },
      "source": [
        "# ðŸ‘¨â€ðŸ”¬ Mr. Owlf POC\n",
        "By Anthony Vilarim Caliani\n",
        "\n",
        "[![#](https://img.shields.io/badge/licence-MIT-lightseagreen.svg)](#) [![#](https://img.shields.io/badge/python-3.7.x-yellow.svg)](#)\n",
        "\n",
        "## Description\n",
        "Mr. Owlf Proof of Concept.\n",
        "\n",
        "\n",
        "## Related Links\n",
        "- [Pantech Solutions: Fake News Detector](https://www.pantechsolutions.net/fake-news-detection-using-machine-learning)\n",
        "- [Towards Data Science: Fake News Detector using NLP](https://towardsdatascience.com/i-built-a-fake-news-detector-using-natural-language-processing-and-classification-models-da180338860e)\n",
        "- [Towards Data Science: Training Fake News Detection AI](https://towardsdatascience.com/i-trained-fake-news-detection-ai-with-95-accuracy-and-almost-went-crazy-d10589aa57c)\n",
        "- [GitHub Project (1)](https://github.com/jfantell/Fake-News-Detection)\n",
        "- [GitHub Project (2)](https://github.com/jasminevasandani/NLP_Classification_Model_FakeNews)\n",
        "\n",
        "---\n",
        "\n",
        "_You can find [@avcaliani](#) at [GitHub](https://github.com/avcaliani) or [GitLab](https://gitlab.com/avcaliani)._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5mm6w8OJAOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "50145ba7-16dc-4066-a567-76de5d6691c3"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9JSAqAeGUE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Tuple, List, Set\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "from pandas import DataFrame, Series, to_datetime, read_csv\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzEuXUSXD05R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  _____        _          _    _ _   _ _     \n",
        "# |  __ \\      | |        | |  | | | (_) |    \n",
        "# | |  | | __ _| |_ __ _  | |  | | |_ _| |___ \n",
        "# | |  | |/ _` | __/ _` | | |  | | __| | / __|\n",
        "# | |__| | (_| | || (_| | | |__| | |_| | \\__ \\\n",
        "# |_____/ \\__,_|\\__\\__,_|  \\____/ \\__|_|_|___/\n",
        "#\n",
        "\n",
        "\n",
        "def read(file_name: str) -> DataFrame:\n",
        "    print(r'+-----------------------------------+')\n",
        "    print(r'|           Reading File            |')\n",
        "    print(r'+-----------------------------------+')\n",
        "\n",
        "    print(f'\\nReading file: \"{file_name}\"')\n",
        "    df = read_csv(file_name)\n",
        "    print(f'Shape: {df.shape}')\n",
        "    print(f'Sample...\\n{df.head(2)}\\n...\\n{df.tail(2)}\\n')\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_data(df: DataFrame) -> None:\n",
        "    print(r'+-----------------------------------+')\n",
        "    print(r'|        Data Frame Clean Up        |')\n",
        "    print(r'+-----------------------------------+')\n",
        "\n",
        "    print(f'Old shape: {df.shape}')\n",
        "    # Drop duplicate rows\n",
        "    df.drop_duplicates(subset='title', inplace=True)\n",
        "    # Remove punctation\n",
        "    df['title'] = df['title'].str.replace(r'[^\\w\\s]', ' ')\n",
        "    # Remove numbers\n",
        "    df['title'] = df['title'].str.replace(r'[^A-Za-z]', ' ')\n",
        "    # Make sure any double-spaces are single\n",
        "    df['title'] = df['title'].str.replace('  ', ' ')\n",
        "    df['title'] = df['title'].str.replace('  ', ' ')\n",
        "    # Transform all text to lowercase\n",
        "    df['title'] = df['title'].str.lower()\n",
        "    # Remove null values\n",
        "    df.dropna(inplace=True)\n",
        "    print(f'New shape: {df.shape}')\n",
        "\n",
        "\n",
        "def show_statistics(df: DataFrame) -> None:\n",
        "    print(r'+-----------------------------------+')\n",
        "    print(r'|       Data Frame Statistics       |')\n",
        "    print(r'+-----------------------------------+')\n",
        "\n",
        "    # Convert Unix Timestamp to Datetime\n",
        "    df['timestamp'] = to_datetime(df['timestamp'], unit='s')\n",
        "    print(f'\\nDate range of posts...')\n",
        "    print(f'* Start date:\\t{df[\"timestamp\"].min()}')\n",
        "    print(f'* End date:\\t{df[\"timestamp\"].max()}')\n",
        "\n",
        "    # Set x values: # of posts\n",
        "    authors: DataFrame = df['author'].value_counts()\n",
        "    authors: DataFrame = authors[authors > 100].sort_values(ascending=False)\n",
        "    print(\n",
        "        f'\\nMost Active Authors...\\n{authors.head(2)}\\n...\\n{authors.tail(2)}\\n')\n",
        "\n",
        "    # Set x values: # of posts\n",
        "    domains: DataFrame = df['domain'].value_counts()\n",
        "    domains: DataFrame = domains.sort_values(ascending=False).head(5)\n",
        "    print(\n",
        "        f'\\nMost referenced domains...\\n{domains.head(2)}\\n...\\n{domains.tail(2)}\\n')\n",
        "\n",
        "\n",
        "#           _____   _    _ _   _ _     \n",
        "#     /\\   |_   _| | |  | | | (_) |    \n",
        "#    /  \\    | |   | |  | | |_ _| |___ \n",
        "#   / /\\ \\   | |   | |  | | __| | / __|\n",
        "#  / ____ \\ _| |_  | |__| | |_| | \\__ \\\n",
        "# /_/    \\_\\_____|  \\____/ \\__|_|_|___/\n",
        "#\n",
        "\n",
        "\n",
        "def count_vectorizer(df: DataFrame, filter_value: int, ngram_range: Tuple[int, int] = (1, 1)) -> DataFrame:\n",
        "    print(r'+-----------------------------------+')\n",
        "    print(r'|         Count Vectorizer          |')\n",
        "    print(r'+-----------------------------------+')\n",
        "\n",
        "    # Set variables to show only one category titles\n",
        "    titles = df[df['subreddit'] == filter_value]['title']\n",
        "\n",
        "    cv = CountVectorizer(stop_words='english', ngram_range=ngram_range)\n",
        "    df_cvec = DataFrame(\n",
        "        # Fit and transform the vectorizer on our corpus\n",
        "        cv.fit_transform(titles).toarray(),\n",
        "        columns=cv.get_feature_names()\n",
        "    )\n",
        "\n",
        "    print(f'Count Vectorizer Result Shape: {df_cvec.shape}')\n",
        "    print(f'Sample...\\n{df_cvec.head(2)}\\n...\\n{df_cvec.tail(2)}\\n')\n",
        "    return df_cvec\n",
        "\n",
        "\n",
        "def unigrams(df: DataFrame, df_2: DataFrame = None) -> Set:\n",
        "    print(r'+-----------------------------------+')\n",
        "    print(r'|             Unigrams              |')\n",
        "    print(r'+-----------------------------------+')\n",
        "\n",
        "    # Set up variables to contain top 5 most used words\n",
        "    df_top_5: Series = df.sum(axis=0).sort_values(ascending=False).head(5)\n",
        "    df_top_5_set = set(df_top_5.index)\n",
        "    print(f'\\nDF:\\n{df_top_5}')\n",
        "\n",
        "    df_2_top_5_set = None\n",
        "    if df_2 is not None:\n",
        "        df_2_top_5: Series = df_2.sum(\n",
        "            axis=0).sort_values(ascending=False).head(5)\n",
        "        df_2_top_5_set = set(df_2_top_5.index)\n",
        "        print(f'\\nDF 2:\\n{df_2_top_5}')\n",
        "\n",
        "    if df_2_top_5_set is not None:\n",
        "        unigrams = df_top_5_set.intersection(df_2_top_5_set)\n",
        "    else:\n",
        "        unigrams = df_top_5_set\n",
        "\n",
        "    print(f'Unigrams: {unigrams}')\n",
        "    return unigrams\n",
        "\n",
        "\n",
        "def get_stop_words(unigrams: List, bigrams: List) -> List:\n",
        "    print(r'+-----------------------------------+')\n",
        "    print(r'|            Stop Words             |')\n",
        "    print(r'+-----------------------------------+')\n",
        "    download('stopwords')\n",
        "    custom = list(stopwords.words('english'))\n",
        "    for i in unigrams:\n",
        "        custom.append(i)\n",
        "\n",
        "    for i in bigrams:\n",
        "        split_words = i.split(\" \")\n",
        "        for word in split_words:\n",
        "            custom.append(word)\n",
        "\n",
        "    print(f'Stop Words: {len(custom)}\\n{custom}')\n",
        "    return custom\n",
        "\n",
        "\n",
        "#           _____   ______         _                   \n",
        "#     /\\   |_   _| |  ____|       | |                  \n",
        "#    /  \\    | |   | |__ __ _  ___| |_ ___  _ __ _   _ \n",
        "#   / /\\ \\   | |   |  __/ _` |/ __| __/ _ \\| '__| | | |\n",
        "#  / ____ \\ _| |_  | | | (_| | (__| || (_) | |  | |_| |\n",
        "# /_/    \\_\\_____| |_|  \\__,_|\\___|\\__\\___/|_|   \\__, |\n",
        "#                                                 __/ |\n",
        "#                                                |___/ \n",
        "\n",
        "\n",
        "class AIFactory:\n",
        "    \"\"\"Algotithms factory!\n",
        "\n",
        "    We are expecting a model that is better than 54% and the majority class is 1 (TheOnion).\n",
        "    If the model is not better than 54%, we know the model is not performing well.\n",
        "\n",
        "    Model 01: Grid Search using 'Count Vectorizer' and 'Logistic Regression'\n",
        "    Model 02: Grid Search using 'Tfidf Vectorizer' and 'Logistic Regression'\n",
        "    Model 03: Grid Search using 'Count Vectorizer' and 'Multinomial Naive Bayes'\n",
        "    Model 04: Grid Search using 'Tfidf Vectorizer' and 'Multinomial Naive Bayes'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x: Series, y: Series, stop_words: List):\n",
        "        self.best_model = None\n",
        "        self.stop_words = stop_words\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            x, y, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "    def get_classifier(self) -> Tuple[any, any, any]:\n",
        "        if self.best_model is not None:\n",
        "            return self.best_model\n",
        "\n",
        "        models = [\n",
        "            self.model_01(),\n",
        "            self.model_02(),\n",
        "            self.model_03(),\n",
        "            self.model_04()\n",
        "        ]\n",
        "        self.best_model = sorted(models, key=lambda v: v[2]['best_score'], reverse=True)[0]\n",
        "\n",
        "        print(f'Best Classifier: {type(self.best_model[0]).__name__}')\n",
        "        print(f'Best Vectorizer: {type(self.best_model[1]).__name__}')\n",
        "\n",
        "        return self.best_model\n",
        "\n",
        "    def model_01(self) -> Tuple[any, any, any]:\n",
        "        print(f'\\n[ LogisticRegression + CountVectorizer ]')\n",
        "        gs = GridSearchCV(\n",
        "            Pipeline([\n",
        "                ('cvec', CountVectorizer()),\n",
        "                ('lr', LogisticRegression(solver='liblinear'))\n",
        "            ]),\n",
        "            param_grid={\n",
        "                'cvec__stop_words': [None, 'english', self.stop_words],\n",
        "                'cvec__ngram_range': [(1, 1), (2, 2), (1, 3)],\n",
        "                'lr__C': [0.01, 1]\n",
        "            },\n",
        "            cv=3\n",
        "        )\n",
        "        gs.fit(self.X_train, self.y_train)\n",
        "\n",
        "        gs_score = self.get_gs_score(gs)\n",
        "        params = gs_score['best_params']\n",
        "\n",
        "        clf = LogisticRegression(C=params['lr__C'], solver='liblinear')\n",
        "        vectorizer = CountVectorizer(\n",
        "            ngram_range=params['cvec__ngram_range'],\n",
        "            stop_words=self.stop_words\n",
        "        )\n",
        "        return clf, vectorizer, gs_score\n",
        "\n",
        "    def model_02(self) -> Tuple[any, any, any]:\n",
        "        print(f'\\n[ LogisticRegression + TfidfVectorizer ]')\n",
        "        gs = GridSearchCV(\n",
        "            Pipeline([\n",
        "                ('tvect', TfidfVectorizer()),\n",
        "                ('lr', LogisticRegression(solver='liblinear'))\n",
        "            ]),\n",
        "            param_grid={\n",
        "                'tvect__max_df': [.75, .98, 1.0],\n",
        "                'tvect__min_df': [2, 3, 5],\n",
        "                'tvect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "                'lr__C': [1]\n",
        "            },\n",
        "            cv=3\n",
        "        )\n",
        "        gs.fit(self.X_train, self.y_train)\n",
        "\n",
        "        gs_score = self.get_gs_score(gs)\n",
        "        params = gs_score['best_params']\n",
        "\n",
        "        clf = LogisticRegression(C=params['lr__C'], solver='liblinear')\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            max_df=params['tvect__max_df'],\n",
        "            min_df=params['tvect__min_df'],\n",
        "            ngram_range=params['tvect__ngram_range'],\n",
        "            stop_words=self.stop_words\n",
        "        )\n",
        "        return clf, vectorizer, gs_score\n",
        "\n",
        "    def model_03(self) -> Tuple[any, any, any]:\n",
        "        print(f'\\n[ MultinomialNB + CountVectorizer ]')\n",
        "        gs = GridSearchCV(\n",
        "            Pipeline([\n",
        "                ('cvec', CountVectorizer()),\n",
        "                ('nb', MultinomialNB())\n",
        "            ]),\n",
        "            param_grid={\n",
        "                'cvec__stop_words': [None, 'english', self.stop_words],\n",
        "                'cvec__ngram_range': [(1, 1), (1, 3)],\n",
        "                'nb__alpha': [.36, .6]\n",
        "            },\n",
        "            cv=3\n",
        "        )\n",
        "        gs.fit(self.X_train, self.y_train)\n",
        "\n",
        "        gs_score = self.get_gs_score(gs)\n",
        "        params = gs_score['best_params']\n",
        "\n",
        "        clf = MultinomialNB(alpha=params['nb__alpha'])\n",
        "        vectorizer = CountVectorizer(\n",
        "            ngram_range=params['cvec__ngram_range'],\n",
        "            stop_words=self.stop_words\n",
        "        )\n",
        "        return clf, vectorizer, gs_score\n",
        "\n",
        "    def model_04(self) -> Tuple[any, any, any]:\n",
        "        print(f'\\n[ MultinomialNB + TfidfVectorizer ]')\n",
        "        gs = GridSearchCV(\n",
        "            Pipeline([\n",
        "                ('tvect', TfidfVectorizer()),\n",
        "                ('nb', MultinomialNB())\n",
        "            ]),\n",
        "            param_grid={\n",
        "                'tvect__max_df': [.75, .98],\n",
        "                'tvect__min_df': [4, 5],\n",
        "                'tvect__ngram_range': [(1, 2), (1, 3)],\n",
        "                'nb__alpha': [0.1, 1]\n",
        "            },\n",
        "            cv=3\n",
        "        )\n",
        "        gs.fit(self.X_train, self.y_train)\n",
        "\n",
        "        gs_score = self.get_gs_score(gs)\n",
        "        params = gs_score['best_params']\n",
        "\n",
        "        clf = MultinomialNB(alpha=params['nb__alpha'])\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            max_df=params['tvect__max_df'],\n",
        "            min_df=params['tvect__min_df'],\n",
        "            ngram_range=params['tvect__ngram_range'],\n",
        "            stop_words=self.stop_words\n",
        "        )\n",
        "        return clf, vectorizer, gs_score\n",
        "\n",
        "    def get_gs_score(self, gs: GridSearchCV) -> any:\n",
        "        score = {\n",
        "            'best_score': round(gs.best_score_ * 100, 2),\n",
        "            'best_params': gs.best_params_,\n",
        "        }\n",
        "\n",
        "        print(f'Best Score  : {score[\"best_score\"]}%')\n",
        "        print(f'Train Score : {round(gs.score(self.X_train, self.y_train) * 100, 2)}%')\n",
        "        print(f'Test Score  : {round(gs.score(self.X_test, self.y_test) * 100, 2)}%')\n",
        "        print(f'Best Params : {score[\"best_params\"]}\\n')\n",
        "        return score\n",
        "\n",
        "\n",
        "#           _____   __  __           _      _ _             \n",
        "#     /\\   |_   _| |  \\/  |         | |    | (_)            \n",
        "#    /  \\    | |   | \\  / | ___   __| | ___| |_ _ __   __ _ \n",
        "#   / /\\ \\   | |   | |\\/| |/ _ \\ / _` |/ _ \\ | | '_ \\ / _` |\n",
        "#  / ____ \\ _| |_  | |  | | (_) | (_| |  __/ | | | | | (_| |\n",
        "# /_/    \\_\\_____| |_|  |_|\\___/ \\__,_|\\___|_|_|_| |_|\\__, |\n",
        "#                                                      __/ |\n",
        "#                                                     |___/ \n",
        "\n",
        "\n",
        "def get_model(df: DataFrame, stop_words: List) -> Tuple[any, any]:\n",
        "    df['subreddit'].value_counts(normalize=True)\n",
        "    x, y = df['title'], df['subreddit']\n",
        "\n",
        "    factory = AIFactory(x, y, stop_words)\n",
        "    clf, vectorizer, gs_score = factory.get_classifier()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42, stratify=y)\n",
        "\n",
        "    vectorizer.fit(X_train)\n",
        "\n",
        "    Xcvec_train = vectorizer.transform(X_train)\n",
        "    Xcvec_test = vectorizer.transform(X_test)\n",
        "\n",
        "    clf.fit(Xcvec_train, y_train)\n",
        "    show_details(clf, vectorizer, gs_score, Xcvec_train, y_train, Xcvec_test, y_test, clf.predict(Xcvec_test))\n",
        "    return clf, vectorizer\n",
        "\n",
        "\n",
        "def show_details(clf, vectorizer, gs_score, Xcvec_train, y_train, Xcvec_test, y_test, preds) -> None:\n",
        "    cnf_matrix = metrics.confusion_matrix(y_test, preds)\n",
        "    tn_fp, fn_tp = np.array(cnf_matrix).tolist()\n",
        "    tn, fp = tn_fp\n",
        "    fn, tp = fn_tp\n",
        "\n",
        "    print(f'Classifier               : {type(clf).__name__}')\n",
        "    print(f'Vectorizer               : {type(vectorizer).__name__}')\n",
        "    print(f'Best Params              : {gs_score[\"best_params\"]}')\n",
        "    print(f'Best Score (Grid Search) : {gs_score[\"best_score\"]}%')\n",
        "    print(f'Train Score              : {round(clf.score(Xcvec_train, y_train) * 100, 2)}%')\n",
        "    print(f'Test Score               : {round(clf.score(Xcvec_test, y_test) * 100, 2)}%')\n",
        "    print(f'Accuracy                 : {round(metrics.accuracy_score(y_test, preds) * 100, 2)}%')\n",
        "    print(f'Precision                : {round(metrics.precision_score(y_test, preds) * 100, 2)}%')\n",
        "    print(f'Recall                   : {round(metrics.recall_score(y_test, preds) * 100, 2)}%')\n",
        "    print(f'Specificity              : {round((tn / (tn + fp)) * 100, 2)}%')\n",
        "    print(f'Misclassification Rate   : {round((fp + fn) / (tn + fp + fn + tn) * 100, 2)}%')\n",
        "    print(f'Confusion Matrix\\n{DataFrame(cnf_matrix).head()}\\n')\n",
        "\n",
        "\n",
        "#           _____   _____                             \n",
        "#     /\\   |_   _| |  __ \\                            \n",
        "#    /  \\    | |   | |__) | __ ___   ___ ___  ___ ___ \n",
        "#   / /\\ \\   | |   |  ___/ '__/ _ \\ / __/ _ \\/ __/ __|\n",
        "#  / ____ \\ _| |_  | |   | | | (_) | (_|  __/\\__ \\__ \\\n",
        "# /_/    \\_\\_____| |_|   |_|  \\___/ \\___\\___||___/___/\n",
        "#\n",
        "\n",
        "\n",
        "class Process:\n",
        "\n",
        "    def __init__(self, clf: any, vectorizer: any):\n",
        "        self.clf = clf\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "    def run(self, sentence: str) -> any:\n",
        "\n",
        "        print(f'Sentence => \"{sentence}\"')\n",
        "        data = DataFrame({'title': [sentence]})\n",
        "\n",
        "        data_cvec = self.vectorizer.transform(data['title'])\n",
        "        preds_prob = self.clf.predict_proba(data_cvec)\n",
        "\n",
        "        fake = '{0:.2f}'.format(preds_prob[0][0])\n",
        "        not_fake = '{0:.2f}'.format(preds_prob[0][1])\n",
        "        print(f'Prob. to be Fake \"{fake}\" / Not Fake \"{not_fake}\"')\n",
        "\n",
        "        return not_fake\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJLx6cF9Iwrs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48ea5020-accf-4278-dd51-da65a1629485"
      },
      "source": [
        "#  __  __       _       \n",
        "# |  \\/  |     (_)      \n",
        "# | \\  / | __ _ _ _ __  \n",
        "# | |\\/| |/ _` | | '_ \\ \n",
        "# | |  | | (_| | | | | |\n",
        "# |_|  |_|\\__,_|_|_| |_|\n",
        "#\n",
        "THE_ONION = f'\\033[1;32;40m[r/The Onion]\\033[0m'\n",
        "NOT_THE_ONION = f'\\033[1;31;40m[r/Not The Onion]\\033[0m'\n",
        "AI = f'\\033[1;35;40m[AI]\\033[0m'\n",
        "ME = f'\\033[1;36;40m[ME]\\033[0m'\n",
        "\n",
        "# Not The Onion\n",
        "print(f'\\n{NOT_THE_ONION}')\n",
        "df_not_onion: DataFrame = read('/content/drive/My Drive/02 - IGTI/TCC/POC/not-the-onion.csv')\n",
        "clean_data(df_not_onion)\n",
        "show_statistics(df_not_onion)\n",
        "\n",
        "# The Onion\n",
        "print(f'\\n{THE_ONION}')\n",
        "df_onion: DataFrame = read('/content/drive/My Drive/02 - IGTI/TCC/POC/the-onion.csv')\n",
        "clean_data(df_onion)\n",
        "show_statistics(df_onion)\n",
        "\n",
        "# Combine df_onion & df_not_onion with only 'subreddit' (target) and 'title' (predictor) columns\n",
        "print(f'\\n{THE_ONION} {NOT_THE_ONION} {\"[Natural Language Processing]\"}')\n",
        "\n",
        "df = pd.concat([df_onion[['subreddit', 'title']], df_not_onion[['subreddit', 'title']]], axis=0)\n",
        "print(f'Combined DF shape: {df.shape}\\n')\n",
        "print(f'Combined DF Sample...\\n{df.head(2)}\\n...\\n{df.tail(2)}\\n\\n')\n",
        "\n",
        "df = df.reset_index(drop=True)  # Reset the index\n",
        "df[\"subreddit\"] = df[\"subreddit\"].map({\"nottheonion\": 0, \"TheOnion\": 1})\n",
        "print(f'Prepared DF Sample...\\n{df.head(2)}\\n...\\n{df.tail(2)}')\n",
        "\n",
        "# Count Vectorize - ngram_range = (1,1)\n",
        "print(f'\\n{THE_ONION}')\n",
        "onion_cvec_df: DataFrame = count_vectorizer(df, filter_value=1)\n",
        "\n",
        "print(f'\\n{NOT_THE_ONION}')\n",
        "not_onion_cvec_df: DataFrame = count_vectorizer(df, filter_value=0)\n",
        "\n",
        "# Unigrams\n",
        "print(f'\\n{THE_ONION} {NOT_THE_ONION}')\n",
        "common_unigrams = list(unigrams(onion_cvec_df, not_onion_cvec_df))\n",
        "\n",
        "# Count Vectorize - ngram_range = (2,2)\n",
        "print(f'\\n{THE_ONION}')\n",
        "onion_cvec_df: DataFrame = count_vectorizer(df, filter_value=1, ngram_range=(2, 2))\n",
        "\n",
        "print(f'\\n{NOT_THE_ONION}')\n",
        "not_onion_cvec_df: DataFrame = count_vectorizer(df, filter_value=0, ngram_range=(2, 2))\n",
        "\n",
        "# Bigrams\n",
        "print(f'\\n{THE_ONION} {NOT_THE_ONION}')\n",
        "common_bigrams = list(unigrams(onion_cvec_df, not_onion_cvec_df))\n",
        "\n",
        "# Stop Words\n",
        "# ------------------\n",
        "# Take out {'man', 'new', 'old', 'people', 'say', 'trump', 'woman', 'year'}\n",
        "# from dataset when modeling, since these words occur frequently in both subreddits.\n",
        "print(f'\\n{THE_ONION} {NOT_THE_ONION}')\n",
        "custom = get_stop_words(common_unigrams, common_bigrams)\n",
        "\n",
        "print(f'\\n{THE_ONION} {NOT_THE_ONION}')\n",
        "clf, vectorizer = get_model(df, custom)\n",
        "\n",
        "sentences = [\n",
        "    'San Diego backyard shed rents for $1,050 a month',\n",
        "    'Are You The Whistleblower? Trump Boys Ask White House Janitor After Giving Him Serum Of All The Sodas Mixed Together',\n",
        "    'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean at diam ac orci pharetra scelerisque non sit amet turpis. Donec quis erat quam',\n",
        "    '12356487984158641351568463213851684132168461'\n",
        "]\n",
        "\n",
        "process = Process(clf, vectorizer)\n",
        "for sentence in sentences:\n",
        "    print(f'\\n{ME}')\n",
        "    process.run(sentence)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31;40m[r/Not The Onion]\u001b[0m\n",
            "+-----------------------------------+\n",
            "|           Reading File            |\n",
            "+-----------------------------------+\n",
            "\n",
            "Reading file: \"/content/drive/My Drive/02 - IGTI/TCC/POC/not-the-onion.csv\"\n",
            "Shape: (15000, 8)\n",
            "Sample...\n",
            "   Unnamed: 0  ...                                              title\n",
            "0           0  ...   San Diego backyard shed rents for $1,050 a month\n",
            "1           1  ...  Orioles players send handwritten thank you not...\n",
            "\n",
            "[2 rows x 8 columns]\n",
            "...\n",
            "       Unnamed: 0  ...                                              title\n",
            "14998       14998  ...  Iowa Official Ousted After Bombarding Staffers...\n",
            "14999       14999  ...  City hopes 'Baby Shark' song will drive homele...\n",
            "\n",
            "[2 rows x 8 columns]\n",
            "\n",
            "+-----------------------------------+\n",
            "|        Data Frame Clean Up        |\n",
            "+-----------------------------------+\n",
            "Old shape: (15000, 8)\n",
            "New shape: (11830, 8)\n",
            "+-----------------------------------+\n",
            "|       Data Frame Statistics       |\n",
            "+-----------------------------------+\n",
            "\n",
            "Date range of posts...\n",
            "* Start date:\t2019-07-17 17:17:32\n",
            "* End date:\t2019-10-03 22:38:38\n",
            "\n",
            "Most Active Authors...\n",
            "[deleted]    112\n",
            "Name: author, dtype: int64\n",
            "...\n",
            "[deleted]    112\n",
            "Name: author, dtype: int64\n",
            "\n",
            "\n",
            "Most referenced domains...\n",
            "theguardian.com    347\n",
            "cnn.com            311\n",
            "Name: domain, dtype: int64\n",
            "...\n",
            "google.com           272\n",
            "independent.co.uk    217\n",
            "Name: domain, dtype: int64\n",
            "\n",
            "\n",
            "\u001b[1;32;40m[r/The Onion]\u001b[0m\n",
            "+-----------------------------------+\n",
            "|           Reading File            |\n",
            "+-----------------------------------+\n",
            "\n",
            "Reading file: \"/content/drive/My Drive/02 - IGTI/TCC/POC/the-onion.csv\"\n",
            "Shape: (15000, 8)\n",
            "Sample...\n",
            "   Unnamed: 0  ...                                              title\n",
            "0           0  ...  Kamala Harris Undergoes Heart Surgery After Se...\n",
            "1           1  ...  6 Pictures Of Scarlett Johansson That Will Mak...\n",
            "\n",
            "[2 rows x 8 columns]\n",
            "...\n",
            "       Unnamed: 0     author        domain  ...  subreddit   timestamp         title\n",
            "14998       14998    Sanlear  theonion.com  ...   TheOnion  1437827079  1437827079.0\n",
            "14999       14999  fakeshits  theonion.com  ...   TheOnion  1437825354  1437825354.0\n",
            "\n",
            "[2 rows x 8 columns]\n",
            "\n",
            "+-----------------------------------+\n",
            "|        Data Frame Clean Up        |\n",
            "+-----------------------------------+\n",
            "Old shape: (15000, 8)\n",
            "New shape: (14226, 8)\n",
            "+-----------------------------------+\n",
            "|       Data Frame Statistics       |\n",
            "+-----------------------------------+\n",
            "\n",
            "Date range of posts...\n",
            "* Start date:\t2015-07-25 11:55:54\n",
            "* End date:\t2019-10-03 20:55:15\n",
            "\n",
            "Most Active Authors...\n",
            "dwaxe               4352\n",
            "GriffonsChainsaw    1312\n",
            "Name: author, dtype: int64\n",
            "...\n",
            "in_hazmat_suit          229\n",
            "residentchubbychaser    107\n",
            "Name: author, dtype: int64\n",
            "\n",
            "\n",
            "Most referenced domains...\n",
            "theonion.com     5850\n",
            "clickhole.com    4188\n",
            "Name: domain, dtype: int64\n",
            "...\n",
            "politics.theonion.com         879\n",
            "entertainment.theonion.com    341\n",
            "Name: domain, dtype: int64\n",
            "\n",
            "\n",
            "\u001b[1;32;40m[r/The Onion]\u001b[0m \u001b[1;31;40m[r/Not The Onion]\u001b[0m [Natural Language Processing]\n",
            "Combined DF shape: (26056, 2)\n",
            "\n",
            "Combined DF Sample...\n",
            "  subreddit                                              title\n",
            "0  TheOnion  kamala harris undergoes heart surgery after se...\n",
            "1  TheOnion   pictures of scarlett johansson that will make...\n",
            "...\n",
            "         subreddit                                              title\n",
            "14995  nottheonion  planned parenthood president forced out after ...\n",
            "14996  nottheonion  police warn of something new to worry about me...\n",
            "\n",
            "\n",
            "Prepared DF Sample...\n",
            "   subreddit                                              title\n",
            "0          1  kamala harris undergoes heart surgery after se...\n",
            "1          1   pictures of scarlett johansson that will make...\n",
            "...\n",
            "       subreddit                                              title\n",
            "26054          0  planned parenthood president forced out after ...\n",
            "26055          0  police warn of something new to worry about me...\n",
            "\n",
            "\u001b[1;32;40m[r/The Onion]\u001b[0m\n",
            "+-----------------------------------+\n",
            "|         Count Vectorizer          |\n",
            "+-----------------------------------+\n",
            "Count Vectorizer Result Shape: (14226, 13647)\n",
            "Sample...\n",
            "   aaron  abandon  abandoned  abandoning  ...  zuckerberg  zumba  zumtrel  zz\n",
            "0      0        0          0           0  ...           0      0        0   0\n",
            "1      0        0          0           0  ...           0      0        0   0\n",
            "\n",
            "[2 rows x 13647 columns]\n",
            "...\n",
            "       aaron  abandon  abandoned  abandoning  ...  zuckerberg  zumba  zumtrel  zz\n",
            "14224      0        0          0           0  ...           0      0        0   0\n",
            "14225      0        0          0           0  ...           0      0        0   0\n",
            "\n",
            "[2 rows x 13647 columns]\n",
            "\n",
            "\n",
            "\u001b[1;31;40m[r/Not The Onion]\u001b[0m\n",
            "+-----------------------------------+\n",
            "|         Count Vectorizer          |\n",
            "+-----------------------------------+\n",
            "Count Vectorizer Result Shape: (11830, 14325)\n",
            "Sample...\n",
            "   aahahahahaha  ...  zurich\n",
            "0             0  ...       0\n",
            "1             0  ...       0\n",
            "\n",
            "[2 rows x 14325 columns]\n",
            "...\n",
            "       aahahahahaha  ...  zurich\n",
            "11828             0  ...       0\n",
            "11829             0  ...       0\n",
            "\n",
            "[2 rows x 14325 columns]\n",
            "\n",
            "\n",
            "\u001b[1;32;40m[r/The Onion]\u001b[0m \u001b[1;31;40m[r/Not The Onion]\u001b[0m\n",
            "+-----------------------------------+\n",
            "|             Unigrams              |\n",
            "+-----------------------------------+\n",
            "\n",
            "DF:\n",
            "new       482\n",
            "man       395\n",
            "just      375\n",
            "trump     370\n",
            "report    196\n",
            "dtype: int64\n",
            "\n",
            "DF 2:\n",
            "man       1249\n",
            "trump      976\n",
            "police     675\n",
            "woman      663\n",
            "says       634\n",
            "dtype: int64\n",
            "Unigrams: {'trump', 'man'}\n",
            "\n",
            "\u001b[1;32;40m[r/The Onion]\u001b[0m\n",
            "+-----------------------------------+\n",
            "|         Count Vectorizer          |\n",
            "+-----------------------------------+\n",
            "Count Vectorizer Result Shape: (14226, 44491)\n",
            "Sample...\n",
            "   aaron eckhart  aaron judge  ...  zumtrel flooby  zz reveals\n",
            "0              0            0  ...               0           0\n",
            "1              0            0  ...               0           0\n",
            "\n",
            "[2 rows x 44491 columns]\n",
            "...\n",
            "       aaron eckhart  aaron judge  ...  zumtrel flooby  zz reveals\n",
            "14224              0            0  ...               0           0\n",
            "14225              0            0  ...               0           0\n",
            "\n",
            "[2 rows x 44491 columns]\n",
            "\n",
            "\n",
            "\u001b[1;31;40m[r/Not The Onion]\u001b[0m\n",
            "+-----------------------------------+\n",
            "|         Count Vectorizer          |\n",
            "+-----------------------------------+\n",
            "Count Vectorizer Result Shape: (11830, 58337)\n",
            "Sample...\n",
            "   aahahahahaha alarm  aaron rodgers  ...  zuckerberg frustrated  zuckerberg wants\n",
            "0                   0              0  ...                      0                 0\n",
            "1                   0              0  ...                      0                 0\n",
            "\n",
            "[2 rows x 58337 columns]\n",
            "...\n",
            "       aahahahahaha alarm  ...  zuckerberg wants\n",
            "11828                   0  ...                 0\n",
            "11829                   0  ...                 0\n",
            "\n",
            "[2 rows x 58337 columns]\n",
            "\n",
            "\n",
            "\u001b[1;32;40m[r/The Onion]\u001b[0m \u001b[1;31;40m[r/Not The Onion]\u001b[0m\n",
            "+-----------------------------------+\n",
            "|             Unigrams              |\n",
            "+-----------------------------------+\n",
            "\n",
            "DF:\n",
            "white house      79\n",
            "year old         78\n",
            "study finds      53\n",
            "week pictures    51\n",
            "pictures week    51\n",
            "dtype: int64\n",
            "\n",
            "DF 2:\n",
            "year old        279\n",
            "donald trump    100\n",
            "florida man      91\n",
            "police say       78\n",
            "man arrested     74\n",
            "dtype: int64\n",
            "Unigrams: {'year old'}\n",
            "\n",
            "\u001b[1;32;40m[r/The Onion]\u001b[0m \u001b[1;31;40m[r/Not The Onion]\u001b[0m\n",
            "+-----------------------------------+\n",
            "|            Stop Words             |\n",
            "+-----------------------------------+\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Stop Words: 183\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'trump', 'man', 'year', 'old']\n",
            "\n",
            "\u001b[1;32;40m[r/The Onion]\u001b[0m \u001b[1;31;40m[r/Not The Onion]\u001b[0m\n",
            "\n",
            "[ LogisticRegression + CountVectorizer ]\n",
            "Best Score  : 84.78%\n",
            "Train Score : 96.09%\n",
            "Test Score  : 85.97%\n",
            "Best Params : {'cvec__ngram_range': (1, 1), 'cvec__stop_words': None, 'lr__C': 1}\n",
            "\n",
            "\n",
            "[ LogisticRegression + TfidfVectorizer ]\n",
            "Best Score  : 83.97%\n",
            "Train Score : 91.2%\n",
            "Test Score  : 85.75%\n",
            "Best Params : {'lr__C': 1, 'tvect__max_df': 0.75, 'tvect__min_df': 3, 'tvect__ngram_range': (1, 3)}\n",
            "\n",
            "\n",
            "[ MultinomialNB + CountVectorizer ]\n",
            "Best Score  : 87.91%\n",
            "Train Score : 99.64%\n",
            "Test Score  : 89.36%\n",
            "Best Params : {'cvec__ngram_range': (1, 3), 'cvec__stop_words': None, 'nb__alpha': 0.36}\n",
            "\n",
            "\n",
            "[ MultinomialNB + TfidfVectorizer ]\n",
            "Best Score  : 85.42%\n",
            "Train Score : 91.85%\n",
            "Test Score  : 85.88%\n",
            "Best Params : {'nb__alpha': 0.1, 'tvect__max_df': 0.75, 'tvect__min_df': 4, 'tvect__ngram_range': (1, 2)}\n",
            "\n",
            "Best Classifier: MultinomialNB\n",
            "Best Vectorizer: CountVectorizer\n",
            "Classifier               : MultinomialNB\n",
            "Vectorizer               : CountVectorizer\n",
            "Best Params              : {'cvec__ngram_range': (1, 3), 'cvec__stop_words': None, 'nb__alpha': 0.36}\n",
            "Best Score (Grid Search) : 87.91%\n",
            "Train Score              : 99.47%\n",
            "Test Score               : 88.55%\n",
            "Accuracy                 : 88.55%\n",
            "Precision                : 89.22%\n",
            "Recall                   : 89.88%\n",
            "Specificity              : 86.95%\n",
            "Misclassification Rate   : 12.67%\n",
            "Confusion Matrix\n",
            "      0     1\n",
            "0  2572   386\n",
            "1   360  3196\n",
            "\n",
            "\n",
            "\u001b[1;36;40m[ME]\u001b[0m\n",
            "Sentence => \"San Diego backyard shed rents for $1,050 a month\"\n",
            "Prob. to be Fake \"1.00\" / Not Fake \"0.00\"\n",
            "\n",
            "\u001b[1;36;40m[ME]\u001b[0m\n",
            "Sentence => \"Are You The Whistleblower? Trump Boys Ask White House Janitor After Giving Him Serum Of All The Sodas Mixed Together\"\n",
            "Prob. to be Fake \"0.00\" / Not Fake \"1.00\"\n",
            "\n",
            "\u001b[1;36;40m[ME]\u001b[0m\n",
            "Sentence => \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean at diam ac orci pharetra scelerisque non sit amet turpis. Donec quis erat quam\"\n",
            "Prob. to be Fake \"0.10\" / Not Fake \"0.90\"\n",
            "\n",
            "\u001b[1;36;40m[ME]\u001b[0m\n",
            "Sentence => \"12356487984158641351568463213851684132168461\"\n",
            "Prob. to be Fake \"0.45\" / Not Fake \"0.55\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}